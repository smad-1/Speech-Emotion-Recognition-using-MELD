🚀 Starting Wav2Vec2 training...
{'loss': 1.3978, 'grad_norm': 4.332315921783447, 'learning_rate': 4.5002123142250534e-05, 'epoch': 1.0}                                                         
{'eval_loss': 1.403122901916504, 'eval_accuracy': 0.2889908256880734, 'eval_runtime': 2.801, 'eval_samples_per_second': 155.661, 'eval_steps_per_second': 19.636, 'epoch': 1.0}                                                                                                                                                 
{'loss': 1.3529, 'grad_norm': 4.532101154327393, 'learning_rate': 4.000212314225053e-05, 'epoch': 2.0}                                                          
{'eval_loss': 1.3921656608581543, 'eval_accuracy': 0.3165137614678899, 'eval_runtime': 2.7939, 'eval_samples_per_second': 156.052, 'eval_steps_per_second': 19.685, 'epoch': 2.0}                                                                                                                                               
{'loss': 1.342, 'grad_norm': 4.00147819519043, 'learning_rate': 3.5002123142250535e-05, 'epoch': 3.0}                                                           
{'eval_loss': 1.3612347841262817, 'eval_accuracy': 0.3440366972477064, 'eval_runtime': 2.7949, 'eval_samples_per_second': 156.0, 'eval_steps_per_second': 19.679, 'epoch': 3.0}                                                                                                                                                 
{'loss': 1.3338, 'grad_norm': 3.509460687637329, 'learning_rate': 3.0002123142250532e-05, 'epoch': 4.0}                                                         
{'eval_loss': 1.5123847723007202, 'eval_accuracy': 0.1743119266055046, 'eval_runtime': 2.6644, 'eval_samples_per_second': 163.642, 'eval_steps_per_second': 20.643, 'epoch': 4.0}                                                                                                                                               
{'loss': 1.3273, 'grad_norm': 5.517404079437256, 'learning_rate': 2.5002123142250532e-05, 'epoch': 5.0}                                                         
{'eval_loss': 1.4179399013519287, 'eval_accuracy': 0.2706422018348624, 'eval_runtime': 2.7737, 'eval_samples_per_second': 157.191, 'eval_steps_per_second': 19.829, 'epoch': 5.0}                                                                                                                                               
{'train_runtime': 3041.7262, 'train_samples_per_second': 61.925, 'train_steps_per_second': 7.742, 'train_loss': 1.3507581940021232, 'epoch': 5.0}               
 50%|██████████████████████████████████████████████████████████▌                                                          | 11775/23550 [50:41<50:41,  3.87it/s]
📊 Evaluating on test set...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 139/139 [00:08<00:00, 17.20it/s]
📊 Final Test Accuracy: {'eval_loss': 1.3424128293991089, 'eval_accuracy': 0.35888187556357076, 'eval_runtime': 8.1267, 'eval_samples_per_second': 136.463, 'eval_steps_per_second': 17.104, 'epoch': 5.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 139/139 [00:08<00:00, 17.19it/s]
✅ Predictions saved to wav2vec2_predictions2.csv
📉 Training curves saved to results2_wav2vec2/






 Starting Wav2Vec2 training for 10 full epochs...
{'loss': 1.3992, 'grad_norm': 3.437019109725952, 'learning_rate': 4.5002123142250534e-05, 'epoch': 1.0}                                                         
{'eval_loss': 1.3838618993759155, 'eval_accuracy': 0.37844036697247707, 'eval_runtime': 2.8912, 'eval_samples_per_second': 150.8, 'eval_steps_per_second': 19.023, 'epoch': 1.0}                                                                                                                                                

{'loss': 1.3526, 'grad_norm': 4.067928791046143, 'learning_rate': 4.000212314225053e-05, 'epoch': 2.0}                                                          
{'eval_loss': 1.3127716779708862, 'eval_accuracy': 0.5137614678899083, 'eval_runtime': 2.9423, 'eval_samples_per_second': 148.184, 'eval_steps_per_second': 18.693, 'epoch': 2.0}                                                                                                                                               

{'loss': 1.3401, 'grad_norm': 3.306621789932251, 'learning_rate': 3.5002123142250535e-05, 'epoch': 3.0}                                                         
{'eval_loss': 1.3483374118804932, 'eval_accuracy': 0.41284403669724773, 'eval_runtime': 2.9441, 'eval_samples_per_second': 148.094, 'eval_steps_per_second': 18.682, 'epoch': 3.0}                                                                                                                                              

{'loss': 1.3339, 'grad_norm': 3.5840823650360107, 'learning_rate': 3.0002123142250532e-05, 'epoch': 4.0}                                                        
{'eval_loss': 1.3863385915756226, 'eval_accuracy': 0.3532110091743119, 'eval_runtime': 2.946, 'eval_samples_per_second': 148.0, 'eval_steps_per_second': 18.67, 'epoch': 4.0}                                                                                                                                                   

{'loss': 1.3273, 'grad_norm': 3.7098844051361084, 'learning_rate': 2.5002123142250532e-05, 'epoch': 5.0}                                                        
{'eval_loss': 1.3721650838851929, 'eval_accuracy': 0.3623853211009174, 'eval_runtime': 2.9444, 'eval_samples_per_second': 148.078, 'eval_steps_per_second': 18.68, 'epoch': 5.0}                                                                                                                                                

{'loss': 1.3215, 'grad_norm': 3.097123861312866, 'learning_rate': 2.0002123142250533e-05, 'epoch': 6.0}                                                         
{'eval_loss': 1.3263720273971558, 'eval_accuracy': 0.4151376146788991, 'eval_runtime': 2.8201, 'eval_samples_per_second': 154.604, 'eval_steps_per_second': 19.503, 'epoch': 6.0}                                                                                                                                               

{'loss': 1.3195, 'grad_norm': 1.6884689331054688, 'learning_rate': 1.5002123142250531e-05, 'epoch': 7.0}                                                        
{'eval_loss': 1.3731005191802979, 'eval_accuracy': 0.3577981651376147, 'eval_runtime': 2.958, 'eval_samples_per_second': 147.396, 'eval_steps_per_second': 18.594, 'epoch': 7.0}                                                                                                                                                

{'loss': 1.3162, 'grad_norm': 3.5811474323272705, 'learning_rate': 1.0002123142250532e-05, 'epoch': 8.0}                                                        
{'eval_loss': 1.3520286083221436, 'eval_accuracy': 0.3761467889908257, 'eval_runtime': 2.9537, 'eval_samples_per_second': 147.61, 'eval_steps_per_second': 18.621, 'epoch': 8.0}                                                                                                                                                

{'loss': 1.3139, 'grad_norm': 3.9908552169799805, 'learning_rate': 5.002123142250531e-06, 'epoch': 9.0}                                                         
{'eval_loss': 1.3766586780548096, 'eval_accuracy': 0.34174311926605505, 'eval_runtime': 2.9418, 'eval_samples_per_second': 148.208, 'eval_steps_per_second': 18.696, 'epoch': 9.0}                                                                                                                                              

{'loss': 1.3141, 'grad_norm': 4.382098197937012, 'learning_rate': 2.1231422505307857e-09, 'epoch': 10.0}                                                        
{'eval_loss': 1.3684734106063843, 'eval_accuracy': 0.36009174311926606, 'eval_runtime': 2.9471, 'eval_samples_per_second': 147.941, 'eval_steps_per_second': 18.662, 'epoch': 10.0}                                                                                                                                             

{'train_runtime': 6320.4966, 'train_samples_per_second': 29.801, 'train_steps_per_second': 3.726, 'train_loss': 1.3338234184248938, 'epoch': 10.0}              
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23550/23550 [1:45:20<00:00,  3.73it/s]
📊 Evaluating on test set...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 139/139 [00:08<00:00, 16.49it/s]
📊 Final Test Accuracy: {'eval_loss': 1.2810344696044922, 'eval_accuracy': 0.5608656447249775, 'eval_runtime': 8.4768, 'eval_samples_per_second': 130.827, 'eval_steps_per_second': 16.398, 'epoch': 10.0}




------
(venv_hubert) (base) bio@cse-System-Product-Name:~/SMS/clone/Speech-Emotion-Recognition-using-MELD$ python wav2vec2_try_better_acc.py
Train class distribution after balancing: Counter({5: 4709, 4: 4709, 6: 4709, 0: 4709})
/home/bio/SMS/clone/Speech-Emotion-Recognition-using-MELD/venv_hubert/lib/python3.11/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/bio/SMS/clone/Speech-Emotion-Recognition-using-MELD/wav2vec2_try_better_acc.py:133: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
🚀 Phase 1: Training classification head only (2 epochs)...
{'loss': 1.4, 'grad_norm': 3.7074882984161377, 'learning_rate': 2.5010615711252656e-05, 'epoch': 1.0}                                                           
{'eval_loss': 1.383642315864563, 'eval_accuracy': 0.3348623853211009, 'eval_runtime': 2.9654, 'eval_samples_per_second': 147.032, 'eval_steps_per_second': 18.548, 'epoch': 1.0}                                                                                                                                                
{'loss': 1.3572, 'grad_norm': 2.4789273738861084, 'learning_rate': 1.0615711252653928e-08, 'epoch': 2.0}                                                        
{'eval_loss': 1.3869609832763672, 'eval_accuracy': 0.3532110091743119, 'eval_runtime': 2.9567, 'eval_samples_per_second': 147.46, 'eval_steps_per_second': 18.602, 'epoch': 2.0}                                                                                                                                                
{'train_runtime': 1252.7625, 'train_samples_per_second': 30.071, 'train_steps_per_second': 3.76, 'train_loss': 1.3786062835887738, 'epoch': 2.0}                
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4710/4710 [20:52<00:00,  3.76it/s]
/home/bio/SMS/clone/Speech-Emotion-Recognition-using-MELD/wav2vec2_try_better_acc.py:180: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
🚀 Phase 2: Fine-tuning full model with stabilizers (10 epochs max)...
 10%|███████████▌                                                                                                        | 1178/11780 [10:37<1:21:05,  2.18it/s][UnfreezeCallback] Unfroze last 2 encoder layers at epoch 1.
{'loss': 1.3532, 'grad_norm': 3.8803415298461914, 'learning_rate': 4.955692029228295e-06, 'epoch': 1.0}                                                         
{'eval_loss': 1.3753896951675415, 'eval_accuracy': 0.3669724770642202, 'eval_runtime': 2.962, 'eval_samples_per_second': 147.197, 'eval_steps_per_second': 18.568, 'epoch': 1.0}                                                                                                                                                
{'loss': 1.3522, 'grad_norm': 2.817293167114258, 'learning_rate': 4.673714395431214e-06, 'epoch': 2.0}                                                          
{'eval_loss': 1.3861652612686157, 'eval_accuracy': 0.3348623853211009, 'eval_runtime': 2.9636, 'eval_samples_per_second': 147.12, 'eval_steps_per_second': 18.559, 'epoch': 2.0}                                                                                                                                                
 30%|██████████████████████████████████▊                                                                                 | 3533/11780 [32:24<1:17:11,  1.78it/s][UnfreezeCallback] Unfroze entire encoder at epoch 3.                                                                                                           
{'loss': 1.3514, 'grad_norm': 3.847442865371704, 'learning_rate': 4.159850242883908e-06, 'epoch': 3.0}                                                          
{'eval_loss': 1.3663783073425293, 'eval_accuracy': 0.38073394495412843, 'eval_runtime': 2.9556, 'eval_samples_per_second': 147.518, 'eval_steps_per_second': 18.609, 'epoch': 3.0}                                                                                                                                              
{'loss': 1.3501, 'grad_norm': 9.226967811584473, 'learning_rate': 3.468917338674446e-06, 'epoch': 4.0}                                                          
{'eval_loss': 1.3651195764541626, 'eval_accuracy': 0.3853211009174312, 'eval_runtime': 2.9705, 'eval_samples_per_second': 146.778, 'eval_steps_per_second': 18.516, 'epoch': 4.0}                                                                                                                                               
{'loss': 1.3522, 'grad_norm': 18.57766342163086, 'learning_rate': 2.674622709900715e-06, 'epoch': 5.0}                                                          
{'eval_loss': 1.36640465259552, 'eval_accuracy': 0.36926605504587157, 'eval_runtime': 2.8282, 'eval_samples_per_second': 154.163, 'eval_steps_per_second': 19.447, 'epoch': 5.0}                                                                                                                                                
{'loss': 1.3501, 'grad_norm': 19.807432174682617, 'learning_rate': 1.8616997588511977e-06, 'epoch': 6.0}                                                        
{'eval_loss': 1.3686658143997192, 'eval_accuracy': 0.3555045871559633, 'eval_runtime': 2.9567, 'eval_samples_per_second': 147.464, 'eval_steps_per_second': 18.602, 'epoch': 6.0}                                                                                                                                               
{'loss': 1.349, 'grad_norm': 8.022814750671387, 'learning_rate': 1.1168691115323468e-06, 'epoch': 7.0}                                                          
{'eval_loss': 1.3655537366867065, 'eval_accuracy': 0.3577981651376147, 'eval_runtime': 2.689, 'eval_samples_per_second': 162.141, 'eval_steps_per_second': 20.454, 'epoch': 7.0}                                                                                                                                                
{'train_runtime': 4912.3495, 'train_samples_per_second': 38.344, 'train_steps_per_second': 2.398, 'train_loss': 1.3511587027753986, 'epoch': 7.0}               
 70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 8246/11780 [1:21:52<35:05,  1.68it/s]
📊 Evaluating on test set...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 139/139 [00:07<00:00, 17.54it/s]
📊 Final Test Accuracy: {'eval_loss': 1.3466490507125854, 'eval_accuracy': 0.43192064923354373, 'eval_runtime': 7.9686, 'eval_samples_per_second': 139.172, 'eval_steps_per_second': 17.444, 'epoch': 7.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 139/139 [00:07<00:00, 17.77it/s]
✅ Predictions saved to wav2vec2_predictions2.csv


----
(venv_hubert) (base) bio@cse-System-Product-Name:~/SMS/clone/Speech-Emotion-Recognition-using-MELD$ python wav2vec2_try_better_acc.py
Train class distribution after balancing: Counter({6: 4709, 5: 4709, 4: 4709, 0: 4709})
/home/bio/SMS/clone/Speech-Emotion-Recognition-using-MELD/venv_hubert/lib/python3.11/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/bio/SMS/clone/Speech-Emotion-Recognition-using-MELD/wav2vec2_try_better_acc.py:133: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
🚀 Phase 1: Training classification head only (2 epochs)...
{'loss': 1.4006, 'grad_norm': 1.0718446969985962, 'learning_rate': 2.5010615711252656e-05, 'epoch': 1.0}                                                        
{'eval_loss': 1.4162662029266357, 'eval_accuracy': 0.25, 'eval_runtime': 2.8313, 'eval_samples_per_second': 153.99, 'eval_steps_per_second': 19.425, 'epoch': 1.0}                                                                                                                                                              
{'loss': 1.3586, 'grad_norm': 3.637401819229126, 'learning_rate': 1.0615711252653928e-08, 'epoch': 2.0}                                                         
{'eval_loss': 1.4007666110992432, 'eval_accuracy': 0.30963302752293576, 'eval_runtime': 2.6853, 'eval_samples_per_second': 162.363, 'eval_steps_per_second': 20.482, 'epoch': 2.0}                                                                                                                                              
{'train_runtime': 1201.1035, 'train_samples_per_second': 31.364, 'train_steps_per_second': 3.921, 'train_loss': 1.3795930574907111, 'epoch': 2.0}               
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4710/4710 [20:01<00:00,  3.92it/s]
/home/bio/SMS/clone/Speech-Emotion-Recognition-using-MELD/wav2vec2_try_better_acc.py:180: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
🚀 Phase 2: Fine-tuning full model with stabilizers (10 epochs max)...
 10%|███████████▌                                                                                                        | 1177/11780 [09:59<1:16:10,  2.32it/s][UnfreezeCallback] Unfroze last 2 encoder layers at epoch 1.
{'loss': 1.3531, 'grad_norm': 1.089067816734314, 'learning_rate': 4.955692029228295e-06, 'epoch': 1.0}                                                          
{'eval_loss': 1.3952138423919678, 'eval_accuracy': 0.30963302752293576, 'eval_runtime': 2.6806, 'eval_samples_per_second': 162.652, 'eval_steps_per_second': 20.518, 'epoch': 1.0}                                                                                                                                              
{'loss': 1.3532, 'grad_norm': 4.2300124168396, 'learning_rate': 4.673714395431214e-06, 'epoch': 2.0}                                                            
{'eval_loss': 1.3954414129257202, 'eval_accuracy': 0.3119266055045872, 'eval_runtime': 2.8872, 'eval_samples_per_second': 151.013, 'eval_steps_per_second': 19.05, 'epoch': 2.0}                                                                                                                                                
 30%|██████████████████████████████████▊                                                                                 | 3533/11780 [31:21<1:15:35,  1.82it/s][UnfreezeCallback] Unfroze entire encoder at epoch 3.                                                                                                           
{'loss': 1.3508, 'grad_norm': 5.390886306762695, 'learning_rate': 4.159850242883908e-06, 'epoch': 3.0}                                                          
{'eval_loss': 1.3797882795333862, 'eval_accuracy': 0.36009174311926606, 'eval_runtime': 2.9527, 'eval_samples_per_second': 147.663, 'eval_steps_per_second': 18.627, 'epoch': 3.0}                                                                                                                                              
{'loss': 1.3494, 'grad_norm': 13.264039039611816, 'learning_rate': 3.468917338674446e-06, 'epoch': 4.0}                                                         
{'eval_loss': 1.3778210878372192, 'eval_accuracy': 0.3577981651376147, 'eval_runtime': 2.9533, 'eval_samples_per_second': 147.632, 'eval_steps_per_second': 18.623, 'epoch': 4.0}                                                                                                                                               
{'loss': 1.3496, 'grad_norm': 20.10940170288086, 'learning_rate': 2.674622709900715e-06, 'epoch': 5.0}                                                          
{'eval_loss': 1.3842260837554932, 'eval_accuracy': 0.3440366972477064, 'eval_runtime': 2.9642, 'eval_samples_per_second': 147.09, 'eval_steps_per_second': 18.555, 'epoch': 5.0}                                                                                                                                                
{'loss': 1.3488, 'grad_norm': 11.118244171142578, 'learning_rate': 1.8616997588511977e-06, 'epoch': 6.0}                                                        
{'eval_loss': 1.382094383239746, 'eval_accuracy': 0.35091743119266056, 'eval_runtime': 2.9573, 'eval_samples_per_second': 147.431, 'eval_steps_per_second': 18.598, 'epoch': 6.0}                                                                                                                                               
{'loss': 1.3479, 'grad_norm': 21.25777244567871, 'learning_rate': 1.1168691115323468e-06, 'epoch': 7.0}                                                         
{'eval_loss': 1.3869256973266602, 'eval_accuracy': 0.3348623853211009, 'eval_runtime': 2.8274, 'eval_samples_per_second': 154.204, 'eval_steps_per_second': 19.452, 'epoch': 7.0}                                                                                                                                               
{'loss': 1.3487, 'grad_norm': 12.957904815673828, 'learning_rate': 5.195874740451412e-07, 'epoch': 8.0}                                                         
{'eval_loss': 1.3854610919952393, 'eval_accuracy': 0.34174311926605505, 'eval_runtime': 2.8113, 'eval_samples_per_second': 155.087, 'eval_steps_per_second': 19.564, 'epoch': 8.0}                                                                                                                                              
{'loss': 1.3492, 'grad_norm': 11.0487060546875, 'learning_rate': 1.335713861159818e-07, 'epoch': 9.0}                                                           
{'eval_loss': 1.3856080770492554, 'eval_accuracy': 0.3463302752293578, 'eval_runtime': 2.9552, 'eval_samples_per_second': 147.536, 'eval_steps_per_second': 18.611, 'epoch': 9.0}                                                                                                                                               
{'loss': 1.3493, 'grad_norm': 30.696321487426758, 'learning_rate': 9.695976549428665e-14, 'epoch': 10.0}                                                        
{'eval_loss': 1.3855284452438354, 'eval_accuracy': 0.3486238532110092, 'eval_runtime': 2.8186, 'eval_samples_per_second': 154.688, 'eval_steps_per_second': 19.513, 'epoch': 10.0}                                                                                                                                              
{'train_runtime': 7053.8366, 'train_samples_per_second': 26.703, 'train_steps_per_second': 1.67, 'train_loss': 1.3499959897104468, 'epoch': 10.0}               
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11780/11780 [1:57:33<00:00,  1.67it/s]
📊 Evaluating on test set...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 139/139 [00:08<00:00, 17.08it/s]
📊 Final Test Accuracy: {'eval_loss': 1.3572049140930176, 'eval_accuracy': 0.3742110009017133, 'eval_runtime': 8.1784, 'eval_samples_per_second': 135.601, 'eval_steps_per_second': 16.996, 'epoch': 10.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 139/139 [00:08<00:00, 17.21it/s]
✅ Predictions saved to wav2vec2_predictions2.csv
📉 Train vs Val loss saved to results2_wav2vec2/train_vs_val_loss_stable.png